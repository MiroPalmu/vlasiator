//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-29373293
// Cuda compilation tools, release 11.2, V11.2.67
// Based on NVVM 7.0.1
//

.version 7.2
.target sm_60
.address_size 64

	// .weak	_Z6minmod10Vec8SimpleIfES0_

.weak .func _Z6minmod10Vec8SimpleIfES0_(
	.param .b64 _Z6minmod10Vec8SimpleIfES0__param_0,
	.param .b64 _Z6minmod10Vec8SimpleIfES0__param_1,
	.param .b64 _Z6minmod10Vec8SimpleIfES0__param_2
)
{
	.reg .pred 	%p<17>;
	.reg .f32 	%f<57>;
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [_Z6minmod10Vec8SimpleIfES0__param_0];
	ld.param.u64 	%rd2, [_Z6minmod10Vec8SimpleIfES0__param_1];
	ld.param.u64 	%rd3, [_Z6minmod10Vec8SimpleIfES0__param_2];
	ld.f32 	%f1, [%rd2];
	abs.f32 	%f2, %f1;
	ld.f32 	%f3, [%rd2+4];
	abs.f32 	%f4, %f3;
	ld.f32 	%f5, [%rd2+8];
	abs.f32 	%f6, %f5;
	ld.f32 	%f7, [%rd2+12];
	abs.f32 	%f8, %f7;
	ld.f32 	%f9, [%rd2+16];
	abs.f32 	%f10, %f9;
	ld.f32 	%f11, [%rd2+20];
	abs.f32 	%f12, %f11;
	ld.f32 	%f13, [%rd2+24];
	abs.f32 	%f14, %f13;
	ld.f32 	%f15, [%rd2+28];
	abs.f32 	%f16, %f15;
	ld.f32 	%f17, [%rd3];
	abs.f32 	%f18, %f17;
	ld.f32 	%f19, [%rd3+4];
	abs.f32 	%f20, %f19;
	ld.f32 	%f21, [%rd3+8];
	abs.f32 	%f22, %f21;
	ld.f32 	%f23, [%rd3+12];
	abs.f32 	%f24, %f23;
	ld.f32 	%f25, [%rd3+16];
	abs.f32 	%f26, %f25;
	ld.f32 	%f27, [%rd3+20];
	abs.f32 	%f28, %f27;
	ld.f32 	%f29, [%rd3+24];
	abs.f32 	%f30, %f29;
	ld.f32 	%f31, [%rd3+28];
	abs.f32 	%f32, %f31;
	setp.geu.ftz.f32 	%p1, %f2, %f18;
	setp.geu.ftz.f32 	%p2, %f4, %f20;
	setp.geu.ftz.f32 	%p3, %f6, %f22;
	setp.geu.ftz.f32 	%p4, %f8, %f24;
	setp.geu.ftz.f32 	%p5, %f10, %f26;
	setp.geu.ftz.f32 	%p6, %f12, %f28;
	setp.geu.ftz.f32 	%p7, %f14, %f30;
	setp.geu.ftz.f32 	%p8, %f16, %f32;
	selp.f32 	%f33, %f17, %f1, %p1;
	selp.f32 	%f34, %f19, %f3, %p2;
	selp.f32 	%f35, %f21, %f5, %p3;
	selp.f32 	%f36, %f23, %f7, %p4;
	selp.f32 	%f37, %f25, %f9, %p5;
	selp.f32 	%f38, %f27, %f11, %p6;
	selp.f32 	%f39, %f29, %f13, %p7;
	selp.f32 	%f40, %f31, %f15, %p8;
	mul.ftz.f32 	%f41, %f17, %f1;
	mul.ftz.f32 	%f42, %f19, %f3;
	mul.ftz.f32 	%f43, %f21, %f5;
	mul.ftz.f32 	%f44, %f23, %f7;
	mul.ftz.f32 	%f45, %f25, %f9;
	mul.ftz.f32 	%f46, %f27, %f11;
	mul.ftz.f32 	%f47, %f29, %f13;
	mul.ftz.f32 	%f48, %f31, %f15;
	setp.gtu.ftz.f32 	%p9, %f41, 0f00000000;
	setp.gtu.ftz.f32 	%p10, %f42, 0f00000000;
	setp.gtu.ftz.f32 	%p11, %f43, 0f00000000;
	setp.gtu.ftz.f32 	%p12, %f44, 0f00000000;
	setp.gtu.ftz.f32 	%p13, %f45, 0f00000000;
	setp.gtu.ftz.f32 	%p14, %f46, 0f00000000;
	setp.gtu.ftz.f32 	%p15, %f47, 0f00000000;
	setp.gtu.ftz.f32 	%p16, %f48, 0f00000000;
	selp.f32 	%f49, %f33, 0f00000000, %p9;
	selp.f32 	%f50, %f34, 0f00000000, %p10;
	selp.f32 	%f51, %f35, 0f00000000, %p11;
	selp.f32 	%f52, %f36, 0f00000000, %p12;
	selp.f32 	%f53, %f37, 0f00000000, %p13;
	selp.f32 	%f54, %f38, 0f00000000, %p14;
	selp.f32 	%f55, %f39, 0f00000000, %p15;
	selp.f32 	%f56, %f40, 0f00000000, %p16;
	st.f32 	[%rd1], %f49;
	st.f32 	[%rd1+4], %f50;
	st.f32 	[%rd1+8], %f51;
	st.f32 	[%rd1+12], %f52;
	st.f32 	[%rd1+16], %f53;
	st.f32 	[%rd1+20], %f54;
	st.f32 	[%rd1+24], %f55;
	st.f32 	[%rd1+28], %f56;
	ret;

}
	// .weak	_Z6maxmod10Vec8SimpleIfES0_
.weak .func _Z6maxmod10Vec8SimpleIfES0_(
	.param .b64 _Z6maxmod10Vec8SimpleIfES0__param_0,
	.param .b64 _Z6maxmod10Vec8SimpleIfES0__param_1,
	.param .b64 _Z6maxmod10Vec8SimpleIfES0__param_2
)
{
	.reg .pred 	%p<17>;
	.reg .f32 	%f<57>;
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [_Z6maxmod10Vec8SimpleIfES0__param_0];
	ld.param.u64 	%rd2, [_Z6maxmod10Vec8SimpleIfES0__param_1];
	ld.param.u64 	%rd3, [_Z6maxmod10Vec8SimpleIfES0__param_2];
	ld.f32 	%f1, [%rd2];
	abs.f32 	%f2, %f1;
	ld.f32 	%f3, [%rd2+4];
	abs.f32 	%f4, %f3;
	ld.f32 	%f5, [%rd2+8];
	abs.f32 	%f6, %f5;
	ld.f32 	%f7, [%rd2+12];
	abs.f32 	%f8, %f7;
	ld.f32 	%f9, [%rd2+16];
	abs.f32 	%f10, %f9;
	ld.f32 	%f11, [%rd2+20];
	abs.f32 	%f12, %f11;
	ld.f32 	%f13, [%rd2+24];
	abs.f32 	%f14, %f13;
	ld.f32 	%f15, [%rd2+28];
	abs.f32 	%f16, %f15;
	ld.f32 	%f17, [%rd3];
	abs.f32 	%f18, %f17;
	ld.f32 	%f19, [%rd3+4];
	abs.f32 	%f20, %f19;
	ld.f32 	%f21, [%rd3+8];
	abs.f32 	%f22, %f21;
	ld.f32 	%f23, [%rd3+12];
	abs.f32 	%f24, %f23;
	ld.f32 	%f25, [%rd3+16];
	abs.f32 	%f26, %f25;
	ld.f32 	%f27, [%rd3+20];
	abs.f32 	%f28, %f27;
	ld.f32 	%f29, [%rd3+24];
	abs.f32 	%f30, %f29;
	ld.f32 	%f31, [%rd3+28];
	abs.f32 	%f32, %f31;
	setp.leu.ftz.f32 	%p1, %f2, %f18;
	setp.leu.ftz.f32 	%p2, %f4, %f20;
	setp.leu.ftz.f32 	%p3, %f6, %f22;
	setp.leu.ftz.f32 	%p4, %f8, %f24;
	setp.leu.ftz.f32 	%p5, %f10, %f26;
	setp.leu.ftz.f32 	%p6, %f12, %f28;
	setp.leu.ftz.f32 	%p7, %f14, %f30;
	setp.leu.ftz.f32 	%p8, %f16, %f32;
	selp.f32 	%f33, %f17, %f1, %p1;
	selp.f32 	%f34, %f19, %f3, %p2;
	selp.f32 	%f35, %f21, %f5, %p3;
	selp.f32 	%f36, %f23, %f7, %p4;
	selp.f32 	%f37, %f25, %f9, %p5;
	selp.f32 	%f38, %f27, %f11, %p6;
	selp.f32 	%f39, %f29, %f13, %p7;
	selp.f32 	%f40, %f31, %f15, %p8;
	mul.ftz.f32 	%f41, %f17, %f1;
	mul.ftz.f32 	%f42, %f19, %f3;
	mul.ftz.f32 	%f43, %f21, %f5;
	mul.ftz.f32 	%f44, %f23, %f7;
	mul.ftz.f32 	%f45, %f25, %f9;
	mul.ftz.f32 	%f46, %f27, %f11;
	mul.ftz.f32 	%f47, %f29, %f13;
	mul.ftz.f32 	%f48, %f31, %f15;
	setp.gtu.ftz.f32 	%p9, %f41, 0f00000000;
	setp.gtu.ftz.f32 	%p10, %f42, 0f00000000;
	setp.gtu.ftz.f32 	%p11, %f43, 0f00000000;
	setp.gtu.ftz.f32 	%p12, %f44, 0f00000000;
	setp.gtu.ftz.f32 	%p13, %f45, 0f00000000;
	setp.gtu.ftz.f32 	%p14, %f46, 0f00000000;
	setp.gtu.ftz.f32 	%p15, %f47, 0f00000000;
	setp.gtu.ftz.f32 	%p16, %f48, 0f00000000;
	selp.f32 	%f49, %f33, 0f00000000, %p9;
	selp.f32 	%f50, %f34, 0f00000000, %p10;
	selp.f32 	%f51, %f35, 0f00000000, %p11;
	selp.f32 	%f52, %f36, 0f00000000, %p12;
	selp.f32 	%f53, %f37, 0f00000000, %p13;
	selp.f32 	%f54, %f38, 0f00000000, %p14;
	selp.f32 	%f55, %f39, 0f00000000, %p15;
	selp.f32 	%f56, %f40, 0f00000000, %p16;
	st.f32 	[%rd1], %f49;
	st.f32 	[%rd1+4], %f50;
	st.f32 	[%rd1+8], %f51;
	st.f32 	[%rd1+12], %f52;
	st.f32 	[%rd1+16], %f53;
	st.f32 	[%rd1+20], %f54;
	st.f32 	[%rd1+24], %f55;
	st.f32 	[%rd1+28], %f56;
	ret;

}
	// .weak	_Z16slope_limiter_sbRK10Vec8SimpleIfES2_S2_
.weak .func _Z16slope_limiter_sbRK10Vec8SimpleIfES2_S2_(
	.param .b64 _Z16slope_limiter_sbRK10Vec8SimpleIfES2_S2__param_0,
	.param .b64 _Z16slope_limiter_sbRK10Vec8SimpleIfES2_S2__param_1,
	.param .b64 _Z16slope_limiter_sbRK10Vec8SimpleIfES2_S2__param_2,
	.param .b64 _Z16slope_limiter_sbRK10Vec8SimpleIfES2_S2__param_3
)
{
	.local .align 4 .b8 	__local_depot2[256];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .f32 	%f<73>;
	.reg .b64 	%rd<105>;


	mov.u64 	%SPL, __local_depot2;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd1, [_Z16slope_limiter_sbRK10Vec8SimpleIfES2_S2__param_0];
	ld.param.u64 	%rd2, [_Z16slope_limiter_sbRK10Vec8SimpleIfES2_S2__param_1];
	ld.param.u64 	%rd3, [_Z16slope_limiter_sbRK10Vec8SimpleIfES2_S2__param_2];
	ld.param.u64 	%rd4, [_Z16slope_limiter_sbRK10Vec8SimpleIfES2_S2__param_3];
	add.u64 	%rd5, %SP, 0;
	add.u64 	%rd6, %SPL, 0;
	add.u64 	%rd7, %SP, 96;
	add.u64 	%rd8, %SPL, 96;
	ld.f32 	%f1, [%rd4];
	ld.f32 	%f2, [%rd3];
	sub.ftz.f32 	%f3, %f1, %f2;
	ld.f32 	%f4, [%rd4+4];
	ld.f32 	%f5, [%rd3+4];
	sub.ftz.f32 	%f6, %f4, %f5;
	ld.f32 	%f7, [%rd4+8];
	ld.f32 	%f8, [%rd3+8];
	sub.ftz.f32 	%f9, %f7, %f8;
	ld.f32 	%f10, [%rd4+12];
	ld.f32 	%f11, [%rd3+12];
	sub.ftz.f32 	%f12, %f10, %f11;
	ld.f32 	%f13, [%rd4+16];
	ld.f32 	%f14, [%rd3+16];
	sub.ftz.f32 	%f15, %f13, %f14;
	ld.f32 	%f16, [%rd4+20];
	ld.f32 	%f17, [%rd3+20];
	sub.ftz.f32 	%f18, %f16, %f17;
	ld.f32 	%f19, [%rd4+24];
	ld.f32 	%f20, [%rd3+24];
	sub.ftz.f32 	%f21, %f19, %f20;
	ld.f32 	%f22, [%rd4+28];
	ld.f32 	%f23, [%rd3+28];
	sub.ftz.f32 	%f24, %f22, %f23;
	ld.f32 	%f25, [%rd2];
	sub.ftz.f32 	%f26, %f2, %f25;
	ld.f32 	%f27, [%rd2+4];
	sub.ftz.f32 	%f28, %f5, %f27;
	ld.f32 	%f29, [%rd2+8];
	sub.ftz.f32 	%f30, %f8, %f29;
	ld.f32 	%f31, [%rd2+12];
	sub.ftz.f32 	%f32, %f11, %f31;
	ld.f32 	%f33, [%rd2+16];
	sub.ftz.f32 	%f34, %f14, %f33;
	ld.f32 	%f35, [%rd2+20];
	sub.ftz.f32 	%f36, %f17, %f35;
	ld.f32 	%f37, [%rd2+24];
	sub.ftz.f32 	%f38, %f20, %f37;
	ld.f32 	%f39, [%rd2+28];
	sub.ftz.f32 	%f40, %f23, %f39;
	add.u64 	%rd9, %SP, 64;
	add.u64 	%rd10, %SPL, 64;
	st.local.f32 	[%rd10], %f3;
	add.s64 	%rd11, %rd9, 4;
	cvta.to.local.u64 	%rd12, %rd11;
	st.local.f32 	[%rd12], %f6;
	add.s64 	%rd13, %rd9, 8;
	cvta.to.local.u64 	%rd14, %rd13;
	st.local.f32 	[%rd14], %f9;
	add.s64 	%rd15, %rd9, 12;
	cvta.to.local.u64 	%rd16, %rd15;
	st.local.f32 	[%rd16], %f12;
	add.s64 	%rd17, %rd9, 16;
	cvta.to.local.u64 	%rd18, %rd17;
	st.local.f32 	[%rd18], %f15;
	add.s64 	%rd19, %rd9, 20;
	cvta.to.local.u64 	%rd20, %rd19;
	st.local.f32 	[%rd20], %f18;
	add.s64 	%rd21, %rd9, 24;
	cvta.to.local.u64 	%rd22, %rd21;
	st.local.f32 	[%rd22], %f21;
	add.s64 	%rd23, %rd9, 28;
	cvta.to.local.u64 	%rd24, %rd23;
	st.local.f32 	[%rd24], %f24;
	add.ftz.f32 	%f41, %f26, %f26;
	add.ftz.f32 	%f42, %f28, %f28;
	add.ftz.f32 	%f43, %f30, %f30;
	add.ftz.f32 	%f44, %f32, %f32;
	add.ftz.f32 	%f45, %f34, %f34;
	add.ftz.f32 	%f46, %f36, %f36;
	add.ftz.f32 	%f47, %f38, %f38;
	add.ftz.f32 	%f48, %f40, %f40;
	st.local.f32 	[%rd6], %f41;
	st.local.f32 	[%rd6+4], %f42;
	st.local.f32 	[%rd6+8], %f43;
	st.local.f32 	[%rd6+12], %f44;
	st.local.f32 	[%rd6+16], %f45;
	st.local.f32 	[%rd6+20], %f46;
	st.local.f32 	[%rd6+24], %f47;
	st.local.f32 	[%rd6+28], %f48;
	add.u64 	%rd25, %SP, 32;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd25;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd9;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd5;
	call.uni 
	_Z6minmod10Vec8SimpleIfES0_, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 0
	add.ftz.f32 	%f49, %f3, %f3;
	add.ftz.f32 	%f50, %f6, %f6;
	add.ftz.f32 	%f51, %f9, %f9;
	add.ftz.f32 	%f52, %f12, %f12;
	add.ftz.f32 	%f53, %f15, %f15;
	add.ftz.f32 	%f54, %f18, %f18;
	add.ftz.f32 	%f55, %f21, %f21;
	add.ftz.f32 	%f56, %f24, %f24;
	st.local.f32 	[%rd8], %f49;
	st.local.f32 	[%rd8+4], %f50;
	st.local.f32 	[%rd8+8], %f51;
	st.local.f32 	[%rd8+12], %f52;
	st.local.f32 	[%rd8+16], %f53;
	st.local.f32 	[%rd8+20], %f54;
	st.local.f32 	[%rd8+24], %f55;
	st.local.f32 	[%rd8+28], %f56;
	add.u64 	%rd26, %SP, 160;
	add.u64 	%rd27, %SPL, 160;
	st.local.f32 	[%rd27], %f26;
	add.s64 	%rd28, %rd26, 4;
	cvta.to.local.u64 	%rd29, %rd28;
	st.local.f32 	[%rd29], %f28;
	add.s64 	%rd30, %rd26, 8;
	cvta.to.local.u64 	%rd31, %rd30;
	st.local.f32 	[%rd31], %f30;
	add.s64 	%rd32, %rd26, 12;
	cvta.to.local.u64 	%rd33, %rd32;
	st.local.f32 	[%rd33], %f32;
	add.s64 	%rd34, %rd26, 16;
	cvta.to.local.u64 	%rd35, %rd34;
	st.local.f32 	[%rd35], %f34;
	add.s64 	%rd36, %rd26, 20;
	cvta.to.local.u64 	%rd37, %rd36;
	st.local.f32 	[%rd37], %f36;
	add.s64 	%rd38, %rd26, 24;
	cvta.to.local.u64 	%rd39, %rd38;
	st.local.f32 	[%rd39], %f38;
	add.s64 	%rd40, %rd26, 28;
	cvta.to.local.u64 	%rd41, %rd40;
	st.local.f32 	[%rd41], %f40;
	add.u64 	%rd42, %SP, 128;
	{ // callseq 1, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd42;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd7;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd26;
	call.uni 
	_Z6minmod10Vec8SimpleIfES0_, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 1
	add.u64 	%rd43, %SPL, 32;
	ld.local.f32 	%f57, [%rd43];
	add.u64 	%rd44, %SP, 192;
	add.u64 	%rd45, %SPL, 192;
	st.local.f32 	[%rd45], %f57;
	add.s64 	%rd46, %rd25, 4;
	cvta.to.local.u64 	%rd47, %rd46;
	ld.local.f32 	%f58, [%rd47];
	add.s64 	%rd48, %rd44, 4;
	cvta.to.local.u64 	%rd49, %rd48;
	st.local.f32 	[%rd49], %f58;
	add.s64 	%rd50, %rd25, 8;
	cvta.to.local.u64 	%rd51, %rd50;
	ld.local.f32 	%f59, [%rd51];
	add.s64 	%rd52, %rd44, 8;
	cvta.to.local.u64 	%rd53, %rd52;
	st.local.f32 	[%rd53], %f59;
	add.s64 	%rd54, %rd25, 12;
	cvta.to.local.u64 	%rd55, %rd54;
	ld.local.f32 	%f60, [%rd55];
	add.s64 	%rd56, %rd44, 12;
	cvta.to.local.u64 	%rd57, %rd56;
	st.local.f32 	[%rd57], %f60;
	add.s64 	%rd58, %rd25, 16;
	cvta.to.local.u64 	%rd59, %rd58;
	ld.local.f32 	%f61, [%rd59];
	add.s64 	%rd60, %rd44, 16;
	cvta.to.local.u64 	%rd61, %rd60;
	st.local.f32 	[%rd61], %f61;
	add.s64 	%rd62, %rd25, 20;
	cvta.to.local.u64 	%rd63, %rd62;
	ld.local.f32 	%f62, [%rd63];
	add.s64 	%rd64, %rd44, 20;
	cvta.to.local.u64 	%rd65, %rd64;
	st.local.f32 	[%rd65], %f62;
	add.s64 	%rd66, %rd25, 24;
	cvta.to.local.u64 	%rd67, %rd66;
	ld.local.f32 	%f63, [%rd67];
	add.s64 	%rd68, %rd44, 24;
	cvta.to.local.u64 	%rd69, %rd68;
	st.local.f32 	[%rd69], %f63;
	add.s64 	%rd70, %rd25, 28;
	cvta.to.local.u64 	%rd71, %rd70;
	ld.local.f32 	%f64, [%rd71];
	add.s64 	%rd72, %rd44, 28;
	cvta.to.local.u64 	%rd73, %rd72;
	st.local.f32 	[%rd73], %f64;
	add.u64 	%rd74, %SPL, 128;
	ld.local.f32 	%f65, [%rd74];
	add.u64 	%rd75, %SP, 224;
	add.u64 	%rd76, %SPL, 224;
	st.local.f32 	[%rd76], %f65;
	add.s64 	%rd77, %rd42, 4;
	cvta.to.local.u64 	%rd78, %rd77;
	ld.local.f32 	%f66, [%rd78];
	add.s64 	%rd79, %rd75, 4;
	cvta.to.local.u64 	%rd80, %rd79;
	st.local.f32 	[%rd80], %f66;
	add.s64 	%rd81, %rd42, 8;
	cvta.to.local.u64 	%rd82, %rd81;
	ld.local.f32 	%f67, [%rd82];
	add.s64 	%rd83, %rd75, 8;
	cvta.to.local.u64 	%rd84, %rd83;
	st.local.f32 	[%rd84], %f67;
	add.s64 	%rd85, %rd42, 12;
	cvta.to.local.u64 	%rd86, %rd85;
	ld.local.f32 	%f68, [%rd86];
	add.s64 	%rd87, %rd75, 12;
	cvta.to.local.u64 	%rd88, %rd87;
	st.local.f32 	[%rd88], %f68;
	add.s64 	%rd89, %rd42, 16;
	cvta.to.local.u64 	%rd90, %rd89;
	ld.local.f32 	%f69, [%rd90];
	add.s64 	%rd91, %rd75, 16;
	cvta.to.local.u64 	%rd92, %rd91;
	st.local.f32 	[%rd92], %f69;
	add.s64 	%rd93, %rd42, 20;
	cvta.to.local.u64 	%rd94, %rd93;
	ld.local.f32 	%f70, [%rd94];
	add.s64 	%rd95, %rd75, 20;
	cvta.to.local.u64 	%rd96, %rd95;
	st.local.f32 	[%rd96], %f70;
	add.s64 	%rd97, %rd42, 24;
	cvta.to.local.u64 	%rd98, %rd97;
	ld.local.f32 	%f71, [%rd98];
	add.s64 	%rd99, %rd75, 24;
	cvta.to.local.u64 	%rd100, %rd99;
	st.local.f32 	[%rd100], %f71;
	add.s64 	%rd101, %rd42, 28;
	cvta.to.local.u64 	%rd102, %rd101;
	ld.local.f32 	%f72, [%rd102];
	add.s64 	%rd103, %rd75, 28;
	cvta.to.local.u64 	%rd104, %rd103;
	st.local.f32 	[%rd104], %f72;
	{ // callseq 2, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd44;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd75;
	call.uni 
	_Z6maxmod10Vec8SimpleIfES0_, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 2
	ret;

}
	// .weak	_Z13slope_limiterRK10Vec8SimpleIfES2_S2_
.weak .func _Z13slope_limiterRK10Vec8SimpleIfES2_S2_(
	.param .b64 _Z13slope_limiterRK10Vec8SimpleIfES2_S2__param_0,
	.param .b64 _Z13slope_limiterRK10Vec8SimpleIfES2_S2__param_1,
	.param .b64 _Z13slope_limiterRK10Vec8SimpleIfES2_S2__param_2,
	.param .b64 _Z13slope_limiterRK10Vec8SimpleIfES2_S2__param_3
)
{
	.local .align 4 .b8 	__local_depot3[256];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .f32 	%f<73>;
	.reg .b64 	%rd<105>;


	mov.u64 	%SPL, __local_depot3;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd1, [_Z13slope_limiterRK10Vec8SimpleIfES2_S2__param_0];
	ld.param.u64 	%rd2, [_Z13slope_limiterRK10Vec8SimpleIfES2_S2__param_1];
	ld.param.u64 	%rd3, [_Z13slope_limiterRK10Vec8SimpleIfES2_S2__param_2];
	ld.param.u64 	%rd4, [_Z13slope_limiterRK10Vec8SimpleIfES2_S2__param_3];
	add.u64 	%rd5, %SP, 0;
	add.u64 	%rd6, %SPL, 0;
	add.u64 	%rd7, %SP, 96;
	add.u64 	%rd8, %SPL, 96;
	ld.f32 	%f1, [%rd4];
	ld.f32 	%f2, [%rd3];
	sub.ftz.f32 	%f3, %f1, %f2;
	ld.f32 	%f4, [%rd4+4];
	ld.f32 	%f5, [%rd3+4];
	sub.ftz.f32 	%f6, %f4, %f5;
	ld.f32 	%f7, [%rd4+8];
	ld.f32 	%f8, [%rd3+8];
	sub.ftz.f32 	%f9, %f7, %f8;
	ld.f32 	%f10, [%rd4+12];
	ld.f32 	%f11, [%rd3+12];
	sub.ftz.f32 	%f12, %f10, %f11;
	ld.f32 	%f13, [%rd4+16];
	ld.f32 	%f14, [%rd3+16];
	sub.ftz.f32 	%f15, %f13, %f14;
	ld.f32 	%f16, [%rd4+20];
	ld.f32 	%f17, [%rd3+20];
	sub.ftz.f32 	%f18, %f16, %f17;
	ld.f32 	%f19, [%rd4+24];
	ld.f32 	%f20, [%rd3+24];
	sub.ftz.f32 	%f21, %f19, %f20;
	ld.f32 	%f22, [%rd4+28];
	ld.f32 	%f23, [%rd3+28];
	sub.ftz.f32 	%f24, %f22, %f23;
	ld.f32 	%f25, [%rd2];
	sub.ftz.f32 	%f26, %f2, %f25;
	ld.f32 	%f27, [%rd2+4];
	sub.ftz.f32 	%f28, %f5, %f27;
	ld.f32 	%f29, [%rd2+8];
	sub.ftz.f32 	%f30, %f8, %f29;
	ld.f32 	%f31, [%rd2+12];
	sub.ftz.f32 	%f32, %f11, %f31;
	ld.f32 	%f33, [%rd2+16];
	sub.ftz.f32 	%f34, %f14, %f33;
	ld.f32 	%f35, [%rd2+20];
	sub.ftz.f32 	%f36, %f17, %f35;
	ld.f32 	%f37, [%rd2+24];
	sub.ftz.f32 	%f38, %f20, %f37;
	ld.f32 	%f39, [%rd2+28];
	sub.ftz.f32 	%f40, %f23, %f39;
	add.u64 	%rd9, %SP, 64;
	add.u64 	%rd10, %SPL, 64;
	st.local.f32 	[%rd10], %f3;
	add.s64 	%rd11, %rd9, 4;
	cvta.to.local.u64 	%rd12, %rd11;
	st.local.f32 	[%rd12], %f6;
	add.s64 	%rd13, %rd9, 8;
	cvta.to.local.u64 	%rd14, %rd13;
	st.local.f32 	[%rd14], %f9;
	add.s64 	%rd15, %rd9, 12;
	cvta.to.local.u64 	%rd16, %rd15;
	st.local.f32 	[%rd16], %f12;
	add.s64 	%rd17, %rd9, 16;
	cvta.to.local.u64 	%rd18, %rd17;
	st.local.f32 	[%rd18], %f15;
	add.s64 	%rd19, %rd9, 20;
	cvta.to.local.u64 	%rd20, %rd19;
	st.local.f32 	[%rd20], %f18;
	add.s64 	%rd21, %rd9, 24;
	cvta.to.local.u64 	%rd22, %rd21;
	st.local.f32 	[%rd22], %f21;
	add.s64 	%rd23, %rd9, 28;
	cvta.to.local.u64 	%rd24, %rd23;
	st.local.f32 	[%rd24], %f24;
	add.ftz.f32 	%f41, %f26, %f26;
	add.ftz.f32 	%f42, %f28, %f28;
	add.ftz.f32 	%f43, %f30, %f30;
	add.ftz.f32 	%f44, %f32, %f32;
	add.ftz.f32 	%f45, %f34, %f34;
	add.ftz.f32 	%f46, %f36, %f36;
	add.ftz.f32 	%f47, %f38, %f38;
	add.ftz.f32 	%f48, %f40, %f40;
	st.local.f32 	[%rd6], %f41;
	st.local.f32 	[%rd6+4], %f42;
	st.local.f32 	[%rd6+8], %f43;
	st.local.f32 	[%rd6+12], %f44;
	st.local.f32 	[%rd6+16], %f45;
	st.local.f32 	[%rd6+20], %f46;
	st.local.f32 	[%rd6+24], %f47;
	st.local.f32 	[%rd6+28], %f48;
	add.u64 	%rd25, %SP, 32;
	{ // callseq 3, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd25;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd9;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd5;
	call.uni 
	_Z6minmod10Vec8SimpleIfES0_, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 3
	add.ftz.f32 	%f49, %f3, %f3;
	add.ftz.f32 	%f50, %f6, %f6;
	add.ftz.f32 	%f51, %f9, %f9;
	add.ftz.f32 	%f52, %f12, %f12;
	add.ftz.f32 	%f53, %f15, %f15;
	add.ftz.f32 	%f54, %f18, %f18;
	add.ftz.f32 	%f55, %f21, %f21;
	add.ftz.f32 	%f56, %f24, %f24;
	st.local.f32 	[%rd8], %f49;
	st.local.f32 	[%rd8+4], %f50;
	st.local.f32 	[%rd8+8], %f51;
	st.local.f32 	[%rd8+12], %f52;
	st.local.f32 	[%rd8+16], %f53;
	st.local.f32 	[%rd8+20], %f54;
	st.local.f32 	[%rd8+24], %f55;
	st.local.f32 	[%rd8+28], %f56;
	add.u64 	%rd26, %SP, 160;
	add.u64 	%rd27, %SPL, 160;
	st.local.f32 	[%rd27], %f26;
	add.s64 	%rd28, %rd26, 4;
	cvta.to.local.u64 	%rd29, %rd28;
	st.local.f32 	[%rd29], %f28;
	add.s64 	%rd30, %rd26, 8;
	cvta.to.local.u64 	%rd31, %rd30;
	st.local.f32 	[%rd31], %f30;
	add.s64 	%rd32, %rd26, 12;
	cvta.to.local.u64 	%rd33, %rd32;
	st.local.f32 	[%rd33], %f32;
	add.s64 	%rd34, %rd26, 16;
	cvta.to.local.u64 	%rd35, %rd34;
	st.local.f32 	[%rd35], %f34;
	add.s64 	%rd36, %rd26, 20;
	cvta.to.local.u64 	%rd37, %rd36;
	st.local.f32 	[%rd37], %f36;
	add.s64 	%rd38, %rd26, 24;
	cvta.to.local.u64 	%rd39, %rd38;
	st.local.f32 	[%rd39], %f38;
	add.s64 	%rd40, %rd26, 28;
	cvta.to.local.u64 	%rd41, %rd40;
	st.local.f32 	[%rd41], %f40;
	add.u64 	%rd42, %SP, 128;
	{ // callseq 4, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd42;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd7;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd26;
	call.uni 
	_Z6minmod10Vec8SimpleIfES0_, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 4
	add.u64 	%rd43, %SPL, 32;
	ld.local.f32 	%f57, [%rd43];
	add.u64 	%rd44, %SP, 192;
	add.u64 	%rd45, %SPL, 192;
	st.local.f32 	[%rd45], %f57;
	add.s64 	%rd46, %rd25, 4;
	cvta.to.local.u64 	%rd47, %rd46;
	ld.local.f32 	%f58, [%rd47];
	add.s64 	%rd48, %rd44, 4;
	cvta.to.local.u64 	%rd49, %rd48;
	st.local.f32 	[%rd49], %f58;
	add.s64 	%rd50, %rd25, 8;
	cvta.to.local.u64 	%rd51, %rd50;
	ld.local.f32 	%f59, [%rd51];
	add.s64 	%rd52, %rd44, 8;
	cvta.to.local.u64 	%rd53, %rd52;
	st.local.f32 	[%rd53], %f59;
	add.s64 	%rd54, %rd25, 12;
	cvta.to.local.u64 	%rd55, %rd54;
	ld.local.f32 	%f60, [%rd55];
	add.s64 	%rd56, %rd44, 12;
	cvta.to.local.u64 	%rd57, %rd56;
	st.local.f32 	[%rd57], %f60;
	add.s64 	%rd58, %rd25, 16;
	cvta.to.local.u64 	%rd59, %rd58;
	ld.local.f32 	%f61, [%rd59];
	add.s64 	%rd60, %rd44, 16;
	cvta.to.local.u64 	%rd61, %rd60;
	st.local.f32 	[%rd61], %f61;
	add.s64 	%rd62, %rd25, 20;
	cvta.to.local.u64 	%rd63, %rd62;
	ld.local.f32 	%f62, [%rd63];
	add.s64 	%rd64, %rd44, 20;
	cvta.to.local.u64 	%rd65, %rd64;
	st.local.f32 	[%rd65], %f62;
	add.s64 	%rd66, %rd25, 24;
	cvta.to.local.u64 	%rd67, %rd66;
	ld.local.f32 	%f63, [%rd67];
	add.s64 	%rd68, %rd44, 24;
	cvta.to.local.u64 	%rd69, %rd68;
	st.local.f32 	[%rd69], %f63;
	add.s64 	%rd70, %rd25, 28;
	cvta.to.local.u64 	%rd71, %rd70;
	ld.local.f32 	%f64, [%rd71];
	add.s64 	%rd72, %rd44, 28;
	cvta.to.local.u64 	%rd73, %rd72;
	st.local.f32 	[%rd73], %f64;
	add.u64 	%rd74, %SPL, 128;
	ld.local.f32 	%f65, [%rd74];
	add.u64 	%rd75, %SP, 224;
	add.u64 	%rd76, %SPL, 224;
	st.local.f32 	[%rd76], %f65;
	add.s64 	%rd77, %rd42, 4;
	cvta.to.local.u64 	%rd78, %rd77;
	ld.local.f32 	%f66, [%rd78];
	add.s64 	%rd79, %rd75, 4;
	cvta.to.local.u64 	%rd80, %rd79;
	st.local.f32 	[%rd80], %f66;
	add.s64 	%rd81, %rd42, 8;
	cvta.to.local.u64 	%rd82, %rd81;
	ld.local.f32 	%f67, [%rd82];
	add.s64 	%rd83, %rd75, 8;
	cvta.to.local.u64 	%rd84, %rd83;
	st.local.f32 	[%rd84], %f67;
	add.s64 	%rd85, %rd42, 12;
	cvta.to.local.u64 	%rd86, %rd85;
	ld.local.f32 	%f68, [%rd86];
	add.s64 	%rd87, %rd75, 12;
	cvta.to.local.u64 	%rd88, %rd87;
	st.local.f32 	[%rd88], %f68;
	add.s64 	%rd89, %rd42, 16;
	cvta.to.local.u64 	%rd90, %rd89;
	ld.local.f32 	%f69, [%rd90];
	add.s64 	%rd91, %rd75, 16;
	cvta.to.local.u64 	%rd92, %rd91;
	st.local.f32 	[%rd92], %f69;
	add.s64 	%rd93, %rd42, 20;
	cvta.to.local.u64 	%rd94, %rd93;
	ld.local.f32 	%f70, [%rd94];
	add.s64 	%rd95, %rd75, 20;
	cvta.to.local.u64 	%rd96, %rd95;
	st.local.f32 	[%rd96], %f70;
	add.s64 	%rd97, %rd42, 24;
	cvta.to.local.u64 	%rd98, %rd97;
	ld.local.f32 	%f71, [%rd98];
	add.s64 	%rd99, %rd75, 24;
	cvta.to.local.u64 	%rd100, %rd99;
	st.local.f32 	[%rd100], %f71;
	add.s64 	%rd101, %rd42, 28;
	cvta.to.local.u64 	%rd102, %rd101;
	ld.local.f32 	%f72, [%rd102];
	add.s64 	%rd103, %rd75, 28;
	cvta.to.local.u64 	%rd104, %rd103;
	st.local.f32 	[%rd104], %f72;
	{ // callseq 5, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd44;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd75;
	call.uni 
	_Z6maxmod10Vec8SimpleIfES0_, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 5
	ret;

}
	// .weak	_ZN10Vec8SimpleIfEC1Ef
.weak .func _ZN10Vec8SimpleIfEC1Ef(
	.param .b64 _ZN10Vec8SimpleIfEC1Ef_param_0,
	.param .b32 _ZN10Vec8SimpleIfEC1Ef_param_1
)
{
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<2>;


	ld.param.u64 	%rd1, [_ZN10Vec8SimpleIfEC1Ef_param_0];
	ld.param.f32 	%f1, [_ZN10Vec8SimpleIfEC1Ef_param_1];
	st.f32 	[%rd1], %f1;
	st.f32 	[%rd1+4], %f1;
	st.f32 	[%rd1+8], %f1;
	st.f32 	[%rd1+12], %f1;
	st.f32 	[%rd1+16], %f1;
	st.f32 	[%rd1+20], %f1;
	st.f32 	[%rd1+24], %f1;
	st.f32 	[%rd1+28], %f1;
	ret;

}
	// .weak	_ZN10Vec8SimpleIfEC1Effffffff
.weak .func _ZN10Vec8SimpleIfEC1Effffffff(
	.param .b64 _ZN10Vec8SimpleIfEC1Effffffff_param_0,
	.param .b32 _ZN10Vec8SimpleIfEC1Effffffff_param_1,
	.param .b32 _ZN10Vec8SimpleIfEC1Effffffff_param_2,
	.param .b32 _ZN10Vec8SimpleIfEC1Effffffff_param_3,
	.param .b32 _ZN10Vec8SimpleIfEC1Effffffff_param_4,
	.param .b32 _ZN10Vec8SimpleIfEC1Effffffff_param_5,
	.param .b32 _ZN10Vec8SimpleIfEC1Effffffff_param_6,
	.param .b32 _ZN10Vec8SimpleIfEC1Effffffff_param_7,
	.param .b32 _ZN10Vec8SimpleIfEC1Effffffff_param_8
)
{
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<2>;


	ld.param.u64 	%rd1, [_ZN10Vec8SimpleIfEC1Effffffff_param_0];
	ld.param.f32 	%f1, [_ZN10Vec8SimpleIfEC1Effffffff_param_1];
	ld.param.f32 	%f2, [_ZN10Vec8SimpleIfEC1Effffffff_param_2];
	ld.param.f32 	%f3, [_ZN10Vec8SimpleIfEC1Effffffff_param_3];
	ld.param.f32 	%f4, [_ZN10Vec8SimpleIfEC1Effffffff_param_4];
	ld.param.f32 	%f5, [_ZN10Vec8SimpleIfEC1Effffffff_param_5];
	ld.param.f32 	%f6, [_ZN10Vec8SimpleIfEC1Effffffff_param_6];
	ld.param.f32 	%f7, [_ZN10Vec8SimpleIfEC1Effffffff_param_7];
	ld.param.f32 	%f8, [_ZN10Vec8SimpleIfEC1Effffffff_param_8];
	st.f32 	[%rd1], %f1;
	st.f32 	[%rd1+4], %f2;
	st.f32 	[%rd1+8], %f3;
	st.f32 	[%rd1+12], %f4;
	st.f32 	[%rd1+16], %f5;
	st.f32 	[%rd1+20], %f6;
	st.f32 	[%rd1+24], %f7;
	st.f32 	[%rd1+28], %f8;
	ret;

}
	// .weak	_ZN10Vec8SimpleIfEC1ERKS0_
.weak .func _ZN10Vec8SimpleIfEC1ERKS0_(
	.param .b64 _ZN10Vec8SimpleIfEC1ERKS0__param_0,
	.param .b64 _ZN10Vec8SimpleIfEC1ERKS0__param_1
)
{
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<3>;


	ld.param.u64 	%rd1, [_ZN10Vec8SimpleIfEC1ERKS0__param_0];
	ld.param.u64 	%rd2, [_ZN10Vec8SimpleIfEC1ERKS0__param_1];
	ld.f32 	%f1, [%rd2];
	st.f32 	[%rd1], %f1;
	ld.f32 	%f2, [%rd2+4];
	st.f32 	[%rd1+4], %f2;
	ld.f32 	%f3, [%rd2+8];
	st.f32 	[%rd1+8], %f3;
	ld.f32 	%f4, [%rd2+12];
	st.f32 	[%rd1+12], %f4;
	ld.f32 	%f5, [%rd2+16];
	st.f32 	[%rd1+16], %f5;
	ld.f32 	%f6, [%rd2+20];
	st.f32 	[%rd1+20], %f6;
	ld.f32 	%f7, [%rd2+24];
	st.f32 	[%rd1+24], %f7;
	ld.f32 	%f8, [%rd2+28];
	st.f32 	[%rd1+28], %f8;
	ret;

}
	// .weak	_ZN10Vec8SimpleIfEaSERKS0_
.weak .func  (.param .b64 func_retval0) _ZN10Vec8SimpleIfEaSERKS0_(
	.param .b64 _ZN10Vec8SimpleIfEaSERKS0__param_0,
	.param .b64 _ZN10Vec8SimpleIfEaSERKS0__param_1
)
{
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<3>;


	ld.param.u64 	%rd1, [_ZN10Vec8SimpleIfEaSERKS0__param_0];
	ld.param.u64 	%rd2, [_ZN10Vec8SimpleIfEaSERKS0__param_1];
	ld.f32 	%f1, [%rd2];
	st.f32 	[%rd1], %f1;
	ld.f32 	%f2, [%rd2+4];
	st.f32 	[%rd1+4], %f2;
	ld.f32 	%f3, [%rd2+8];
	st.f32 	[%rd1+8], %f3;
	ld.f32 	%f4, [%rd2+12];
	st.f32 	[%rd1+12], %f4;
	ld.f32 	%f5, [%rd2+16];
	st.f32 	[%rd1+16], %f5;
	ld.f32 	%f6, [%rd2+20];
	st.f32 	[%rd1+20], %f6;
	ld.f32 	%f7, [%rd2+24];
	st.f32 	[%rd1+24], %f7;
	ld.f32 	%f8, [%rd2+28];
	st.f32 	[%rd1+28], %f8;
	st.param.b64 	[func_retval0+0], %rd1;
	ret;

}
	// .weak	_ZN10Vec8SimpleIbEC1Ebbbbbbbb
.weak .func _ZN10Vec8SimpleIbEC1Ebbbbbbbb(
	.param .b64 _ZN10Vec8SimpleIbEC1Ebbbbbbbb_param_0,
	.param .b32 _ZN10Vec8SimpleIbEC1Ebbbbbbbb_param_1,
	.param .b32 _ZN10Vec8SimpleIbEC1Ebbbbbbbb_param_2,
	.param .b32 _ZN10Vec8SimpleIbEC1Ebbbbbbbb_param_3,
	.param .b32 _ZN10Vec8SimpleIbEC1Ebbbbbbbb_param_4,
	.param .b32 _ZN10Vec8SimpleIbEC1Ebbbbbbbb_param_5,
	.param .b32 _ZN10Vec8SimpleIbEC1Ebbbbbbbb_param_6,
	.param .b32 _ZN10Vec8SimpleIbEC1Ebbbbbbbb_param_7,
	.param .b32 _ZN10Vec8SimpleIbEC1Ebbbbbbbb_param_8
)
{
	.reg .b16 	%rs<9>;
	.reg .b64 	%rd<2>;


	ld.param.u64 	%rd1, [_ZN10Vec8SimpleIbEC1Ebbbbbbbb_param_0];
	ld.param.u8 	%rs1, [_ZN10Vec8SimpleIbEC1Ebbbbbbbb_param_1];
	ld.param.u8 	%rs2, [_ZN10Vec8SimpleIbEC1Ebbbbbbbb_param_2];
	ld.param.u8 	%rs3, [_ZN10Vec8SimpleIbEC1Ebbbbbbbb_param_3];
	ld.param.u8 	%rs4, [_ZN10Vec8SimpleIbEC1Ebbbbbbbb_param_4];
	ld.param.u8 	%rs5, [_ZN10Vec8SimpleIbEC1Ebbbbbbbb_param_5];
	ld.param.u8 	%rs6, [_ZN10Vec8SimpleIbEC1Ebbbbbbbb_param_6];
	ld.param.u8 	%rs7, [_ZN10Vec8SimpleIbEC1Ebbbbbbbb_param_7];
	ld.param.u8 	%rs8, [_ZN10Vec8SimpleIbEC1Ebbbbbbbb_param_8];
	st.u8 	[%rd1], %rs1;
	st.u8 	[%rd1+1], %rs2;
	st.u8 	[%rd1+2], %rs3;
	st.u8 	[%rd1+3], %rs4;
	st.u8 	[%rd1+4], %rs5;
	st.u8 	[%rd1+5], %rs6;
	st.u8 	[%rd1+6], %rs7;
	st.u8 	[%rd1+7], %rs8;
	ret;

}
	// .weak	_ZSt4fabsf
.weak .func  (.param .b32 func_retval0) _ZSt4fabsf(
	.param .b32 _ZSt4fabsf_param_0
)
{
	.reg .f32 	%f<3>;


	ld.param.f32 	%f1, [_ZSt4fabsf_param_0];
	abs.f32 	%f2, %f1;
	st.param.f32 	[func_retval0+0], %f2;
	ret;

}

